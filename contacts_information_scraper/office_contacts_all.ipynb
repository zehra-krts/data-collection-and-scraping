{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_driver(headless: bool = False) -> webdriver.Chrome:\n",
    " \n",
    "    options = webdriver.ChromeOptions()\n",
    "    \n",
    "    # Example flags that help scraping stability\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "\n",
    "driver = setup_driver(headless=False)\n",
    "wait = WebDriverWait(driver, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://example-directory.com\"\n",
    "TEST_DETAIL_URL = f\"{BASE_URL}/members/12345\"\n",
    "\n",
    "\n",
    "def is_access_allowed(url: str) -> str:\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        WebDriverWait(driver, 6).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "\n",
    "    html = driver.page_source.lower()\n",
    "\n",
    "    if \"access has been restricted\" in html or \"exceeding the limit\" in html:\n",
    "        return \"RESTRICTED\"\n",
    "\n",
    "    if \"members only\" in html or \"please login\" in html:\n",
    "        return \"LOGIN_REQUIRED\"\n",
    "\n",
    "    return \"OK\"\n",
    "\n",
    "\n",
    "print(\"Test access:\", is_access_allowed(TEST_DETAIL_URL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input: members_raw.csv with at least a 'url' column\n",
    "INPUT_MEMBERS_CSV = \"members_raw.csv\"\n",
    "\n",
    "df_links = pd.read_csv(INPUT_MEMBERS_CSV)\n",
    "\n",
    "# Normalize URLs\n",
    "df_links[\"url\"] = (\n",
    "    df_links[\"url\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.split(\"#\").str[0]\n",
    "    .str.rstrip(\"/\")\n",
    ")\n",
    "\n",
    "URLS = (\n",
    "    df_links[\"url\"]\n",
    "    .dropna()\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .loc[lambda s: s.str.startswith(\"http\")]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "print(\"Total URL count:\", len(URLS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_office_contacts(url: str):\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for body to load\n",
    "    try:\n",
    "        WebDriverWait(driver, 6).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        return []\n",
    "\n",
    "    html = driver.page_source\n",
    "    html_lower = html.lower()\n",
    "\n",
    "    # Access restriction checks\n",
    "    if \"access has been restricted\" in html_lower or \"exceeding the limit\" in html_lower:\n",
    "        print(\"ACCESS RESTRICTED for:\", url)\n",
    "        return []\n",
    "\n",
    "    if \"members only\" in html_lower or \"please login\" in html_lower:\n",
    "        print(\"MEMBERS ONLY page:\", url)\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    contacts = []\n",
    "\n",
    "    \n",
    "    contact_cards = soup.select(\"div.contact-card\")\n",
    "    if not contact_cards:\n",
    "        # Fallback: maybe the info lives in a definition list\n",
    "        contact_cards = soup.select(\"div.office-contact\")\n",
    "\n",
    "    for card in contact_cards:\n",
    "        contact = {}\n",
    "       \n",
    "        for row in card.select(\"dl\"):\n",
    "            label_el = row.find(\"dt\")\n",
    "            value_el = row.find(\"dd\")\n",
    "\n",
    "            if not label_el or not value_el:\n",
    "                continue\n",
    "\n",
    "            label = label_el.get_text(\" \", strip=True)\n",
    "            value = value_el.get_text(\" \", strip=True)\n",
    "\n",
    "            if \"members only\" in value.lower():\n",
    "                value = None\n",
    "\n",
    "            if label:\n",
    "                contact[label] = value\n",
    "\n",
    "        if contact:\n",
    "            contacts.append(contact)\n",
    "\n",
    "    return contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_WIDE_CSV = \"office_contacts_wide.csv\"\n",
    "OUT_LONG_CSV = \"office_contacts_long.csv\"\n",
    "LOG_CSV = \"scrape_log.csv\"\n",
    "\n",
    "MAX_CONTACT = 8\n",
    "GLOBAL_LABELS = [\"Name\", \"Title\", \"Direct Line\", \"Email\", \"Mobile\", \"Fax\"]\n",
    "\n",
    "\n",
    "def normalize_label(label: str) -> str:\n",
    "    return label.replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_log(i: int, url: str, status: str, message: str = \"\"):\n",
    "    row = {\"i\": i, \"url\": url, \"status\": status, \"message\": message}\n",
    "    df_row = pd.DataFrame([row])\n",
    "\n",
    "    if not os.path.exists(LOG_CSV):\n",
    "        df_row.to_csv(LOG_CSV, index=False)\n",
    "    else:\n",
    "        df_row.to_csv(LOG_CSV, index=False, mode=\"a\", header=False)\n",
    "\n",
    "\n",
    "def get_resume_index() -> int:\n",
    "    if not os.path.exists(LOG_CSV):\n",
    "        return 0\n",
    "\n",
    "    df_log = pd.read_csv(LOG_CSV)\n",
    "    if df_log.empty:\n",
    "        return 0\n",
    "\n",
    "    success_rows = df_log[df_log[\"status\"] == \"SUCCESS\"]\n",
    "    if success_rows.empty:\n",
    "        return 0\n",
    "\n",
    "    return int(success_rows[\"i\"].max())\n",
    "\n",
    "\n",
    "resume_from = get_resume_index()\n",
    "print(\"Resuming from index:\", resume_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "failed_urls = []\n",
    "\n",
    "# Determine if we need to write header\n",
    "write_header = not os.path.exists(OUT_WIDE_CSV)\n",
    "processed = 0\n",
    "\n",
    "for i, url in enumerate(URLS, 1):\n",
    "    if i <= resume_from:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        contacts = scrape_office_contacts(url)\n",
    "\n",
    "        # Build a wide-format row: url + c1_Name, c1_Email, ..., cN_Fax\n",
    "        wide_row = {\"url\": url}\n",
    "        for idx, contact in enumerate(contacts[:MAX_CONTACT], start=1):\n",
    "            for label in GLOBAL_LABELS:\n",
    "                key = f\"c{idx}_{normalize_label(label)}\"\n",
    "                value = contact.get(label)\n",
    "                wide_row[key] = value\n",
    "\n",
    "        records.append(wide_row)\n",
    "\n",
    "        df_chunk = pd.DataFrame(records)\n",
    "        df_chunk.to_csv(OUT_WIDE_CSV, index=False, mode=\"a\", header=write_header)\n",
    "\n",
    "        write_header = False\n",
    "        records.clear()\n",
    "\n",
    "        append_log(i, url, \"SUCCESS\", \"\")\n",
    "        processed += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        failed_urls.append({\"url\": url, \"error\": str(e)})\n",
    "        append_log(i, url, \"FAILED\", str(e))\n",
    "        print(\"FAILED:\", url, \"->\", e)\n",
    "\n",
    "    # Simple rate limiting\n",
    "    time.sleep(random.uniform(3.0, 6.0))\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"{i}/{len(URLS)} processed in this run:\", processed)\n",
    "        time.sleep(random.uniform(20, 40))\n",
    "\n",
    "print(\"Successful URLs in this run:\", processed)\n",
    "print(\"Failed URLs in this run:\", len(failed_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wide table\n",
    "df_wide = pd.read_csv(OUT_WIDE_CSV)\n",
    "\n",
    "# Optional: merge company_name from the original links file\n",
    "df_comp_unique = (\n",
    "    df_links[[\"url\", \"company_name\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Build long table: one row per contact field\n",
    "value_cols = [c for c in df_wide.columns if c != \"url\"]\n",
    "\n",
    "df_long = (\n",
    "    df_wide\n",
    "    .set_index(\"url\")[value_cols]\n",
    "    .stack()\n",
    "    .reset_index(name=\"value\")\n",
    ")\n",
    "\n",
    "df_long.columns = [\"url\", \"contact_field\", \"value\"]\n",
    "\n",
    "# Extract contact index and field name from columns like \"c1_Name\"\n",
    "df_long[\"contact_index\"] = (\n",
    "    df_long[\"contact_field\"]\n",
    "    .str.extract(r\"c(\\d+)_\")[0]\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "df_long[\"field_name\"] = (\n",
    "    df_long[\"contact_field\"]\n",
    "    .str.replace(r\"c\\d+_\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "df_long = df_long.dropna(subset=[\"value\"]).reset_index(drop=True)\n",
    "\n",
    "# Merge company_name\n",
    "df_long_with_name = df_long.merge(\n",
    "    df_comp_unique[[\"url\", \"company_name\"]],\n",
    "    on=\"url\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Reorder columns\n",
    "cols = df_long_with_name.columns.tolist()\n",
    "new_cols = (\n",
    "    [\"company_name\"] +\n",
    "    [c for c in cols if c not in [\"company_name\", \"url\"]] +\n",
    "    [\"url\"]\n",
    ")\n",
    "\n",
    "df_long_reordered = df_long_with_name[new_cols]\n",
    "\n",
    "df_long_reordered.to_csv(OUT_LONG_CSV, index=False)\n",
    "df_long_reordered.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
